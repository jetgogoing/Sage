#!/usr/bin/env python3
"""
Sage MCP æç¤ºå¢å¼ºå¼•æ“ V2.0
ğŸ¯ ä¸–ç•Œçº§æç¤ºå·¥ç¨‹ï¼Œèåˆè®°å¿†ã€ä¸Šä¸‹æ–‡å’Œæ™ºèƒ½æ¨¡æ¿ç³»ç»Ÿ
"""

import asyncio
import json
import re
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
import logging

from intelligent_retrieval import (
    IntelligentRetrievalEngine, 
    QueryType, 
    RetrievalStrategy,
    RetrievalResult
)

logger = logging.getLogger('SagePromptEnhancer')


class EnhancementLevel(Enum):
    """å¢å¼ºçº§åˆ«"""
    MINIMAL = "minimal"        # æœ€å°å¢å¼º
    STANDARD = "standard"      # æ ‡å‡†å¢å¼º
    COMPREHENSIVE = "comprehensive"  # å…¨é¢å¢å¼º
    ADAPTIVE = "adaptive"      # è‡ªé€‚åº”å¢å¼º


class PromptType(Enum):
    """æç¤ºç±»å‹"""
    CODING = "coding"          # ç¼–ç¨‹ç›¸å…³
    DEBUGGING = "debugging"    # è°ƒè¯•ç›¸å…³
    EXPLANATION = "explanation"  # è§£é‡Šè¯´æ˜
    ANALYSIS = "analysis"      # åˆ†æç±»
    CREATIVE = "creative"      # åˆ›æ„ç±»
    GENERAL = "general"        # é€šç”¨ç±»


@dataclass
class EnhancementContext:
    """å¢å¼ºä¸Šä¸‹æ–‡"""
    original_prompt: str
    enhanced_prompt: str
    fragments_used: List[RetrievalResult]
    enhancement_reasoning: str
    metadata: Dict[str, Any]
    confidence_score: float = 0.0


class MemoryFusionProcessor:
    """è®°å¿†èåˆå¤„ç†å™¨ - åŸºäºç”¨æˆ·æ¨¡æ¿"""
    
    def __init__(self, prompts_dir: Path):
        self.prompts_dir = prompts_dir
        self.template_cache = {}
        self._load_templates()
        
    def _load_templates(self):
        """åŠ è½½ç”¨æˆ·æç¤ºæ¨¡æ¿"""
        try:
            template_file = self.prompts_dir / "memory_fusion_prompt_programming.txt"
            if template_file.exists():
                with open(template_file, 'r', encoding='utf-8') as f:
                    self.user_template = f.read()
                logger.info(f"å·²åŠ è½½ç”¨æˆ·æ¨¡æ¿: {template_file}")
            else:
                logger.warning(f"ç”¨æˆ·æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {template_file}")
                self.user_template = None
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡æ¿å¤±è´¥: {e}")
            self.user_template = None
    
    def process_memory_fusion(
        self,
        fragments: List[RetrievalResult],
        max_tokens: int = 3000
    ) -> str:
        """ä½¿ç”¨ç”¨æˆ·æ¨¡æ¿å¤„ç†è®°å¿†èåˆ"""
        if not self.user_template:
            return self._fallback_fusion(fragments, max_tokens)
            
        # æ„å»ºæ£€ç´¢ç‰‡æ®µæ–‡æœ¬
        retrieved_passages = []
        for i, fragment in enumerate(fragments, 1):
            role_label = "ç”¨æˆ·" if fragment.role == "user" else "åŠ©æ‰‹"
            passage = f"<fragment_{i:02d}>\n[{role_label}] {fragment.content}\n</fragment_{i:02d}>"
            retrieved_passages.append(passage)
            
        passages_text = "\n\n".join(retrieved_passages)
        
        # æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦
        enhanced_template = self.user_template.replace(
            "{retrieved_passages}", 
            passages_text
        )
        
        # æˆªæ–­ä»¥æ»¡è¶³tokené™åˆ¶
        if len(enhanced_template) > max_tokens * 4:  # ç²—ç•¥ä¼°ç®—
            enhanced_template = enhanced_template[:max_tokens * 4] + "\n...\n"
            
        return enhanced_template
    
    def _fallback_fusion(self, fragments: List[RetrievalResult], max_tokens: int) -> str:
        """å¤‡ç”¨èåˆæ–¹æ³•"""
        if not fragments:
            return ""
            
        parts = ["## ç›¸å…³è®°å¿†ç‰‡æ®µ"]
        token_count = 0
        
        for i, fragment in enumerate(fragments, 1):
            role = "ç”¨æˆ·" if fragment.role == "user" else "åŠ©æ‰‹"
            content = fragment.content
            
            # ä¼°ç®—tokenå¹¶æˆªæ–­
            estimated_tokens = len(content) // 4
            if token_count + estimated_tokens > max_tokens:
                remaining_tokens = max_tokens - token_count
                if remaining_tokens > 50:
                    content = content[:remaining_tokens * 4] + "..."
                else:
                    break
                    
            parts.append(f"\n### ç‰‡æ®µ {i} ({role})")
            parts.append(content)
            parts.append(f"*ç›¸å…³æ€§: {fragment.final_score:.2f} | {fragment.reasoning}*")
            
            token_count += estimated_tokens
            
        return "\n".join(parts)


class AdaptivePromptGenerator:
    """è‡ªé€‚åº”æç¤ºç”Ÿæˆå™¨"""
    
    def __init__(self):
        # æç¤ºæ¨¡å¼åº“
        self.prompt_patterns = {
            PromptType.CODING: {
                'prefix': "ä½œä¸ºèµ„æ·±è½¯ä»¶å·¥ç¨‹å¸ˆï¼ŒåŸºäºä»¥ä¸‹æŠ€æœ¯èƒŒæ™¯ï¼š",
                'context_label': "## ç›¸å…³æŠ€æœ¯èƒŒæ™¯",
                'task_label': "## å½“å‰å¼€å‘ä»»åŠ¡",
                'suffix': "è¯·æä¾›å…·ä½“ã€å¯æ‰§è¡Œçš„ä»£ç è§£å†³æ–¹æ¡ˆï¼ŒåŒ…å«å¿…è¦çš„æ³¨é‡Šå’Œé”™è¯¯å¤„ç†ã€‚"
            },
            PromptType.DEBUGGING: {
                'prefix': "ä½œä¸ºè°ƒè¯•ä¸“å®¶ï¼Œå‚è€ƒä»¥ä¸‹é—®é¢˜è§£å†³å†å²ï¼š",
                'context_label': "## ç±»ä¼¼é—®é¢˜è§£å†³è®°å½•",
                'task_label': "## å½“å‰è°ƒè¯•ä»»åŠ¡",
                'suffix': "è¯·åˆ†æé—®é¢˜æ ¹å› ï¼Œæä¾›ç³»ç»Ÿæ€§çš„è°ƒè¯•æ–¹æ¡ˆå’Œé¢„é˜²æªæ–½ã€‚"
            },
            PromptType.EXPLANATION: {
                'prefix': "åŸºäºä»¥ä¸‹ç›¸å…³çŸ¥è¯†èƒŒæ™¯ï¼š",
                'context_label': "## ç›¸å…³æ¦‚å¿µå’ŒèƒŒæ™¯",
                'task_label': "## éœ€è¦è§£é‡Šçš„é—®é¢˜",
                'suffix': "è¯·æä¾›æ¸…æ™°ã€ç»“æ„åŒ–çš„è§£é‡Šï¼Œä½¿ç”¨æ°å½“çš„ç¤ºä¾‹å’Œç±»æ¯”ã€‚"
            },
            PromptType.ANALYSIS: {
                'prefix': "ä½œä¸ºåˆ†æå¸ˆï¼Œç»“åˆä»¥ä¸‹å‚è€ƒä¿¡æ¯ï¼š",
                'context_label': "## ç›¸å…³åˆ†ææ¡ˆä¾‹",
                'task_label': "## å½“å‰åˆ†æç›®æ ‡",
                'suffix': "è¯·è¿›è¡Œæ·±å…¥åˆ†æï¼Œæä¾›æ•°æ®æ”¯æ’‘çš„ç»“è®ºå’Œå»ºè®®ã€‚"
            }
        }
        
    def generate_adaptive_prompt(
        self,
        original_prompt: str,
        context_content: str,
        prompt_type: PromptType,
        enhancement_level: EnhancementLevel
    ) -> str:
        """ç”Ÿæˆè‡ªé€‚åº”æç¤º"""
        
        if enhancement_level == EnhancementLevel.MINIMAL:
            return f"{context_content}\n\n{original_prompt}"
            
        pattern = self.prompt_patterns.get(prompt_type, self.prompt_patterns[PromptType.CODING])
        
        parts = []
        
        # æ·»åŠ å‰ç¼€
        if enhancement_level in [EnhancementLevel.COMPREHENSIVE, EnhancementLevel.ADAPTIVE]:
            parts.append(pattern['prefix'])
            
        # æ·»åŠ ä¸Šä¸‹æ–‡
        if context_content.strip():
            parts.append(f"\n{pattern['context_label']}")
            parts.append(context_content)
            
        # æ·»åŠ ä»»åŠ¡
        parts.append(f"\n{pattern['task_label']}")
        parts.append(original_prompt)
        
        # æ·»åŠ åç¼€æŒ‡å¯¼
        if enhancement_level == EnhancementLevel.COMPREHENSIVE:
            parts.append(f"\n{pattern['suffix']}")
            
        return "\n".join(parts)


class IntelligentPromptEnhancer:
    """æ™ºèƒ½æç¤ºå¢å¼ºå™¨ - ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self, retrieval_engine: IntelligentRetrievalEngine, prompts_dir: Path):
        self.retrieval_engine = retrieval_engine
        self.memory_fusion = MemoryFusionProcessor(prompts_dir)
        self.adaptive_generator = AdaptivePromptGenerator()
        
        # å¢å¼ºé…ç½®
        self.config = {
            'default_enhancement_level': EnhancementLevel.ADAPTIVE,
            'max_context_tokens': 2000,
            'min_relevance_threshold': 0.3,
            'diversity_boost': True,
            'confidence_threshold': 0.6
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_enhancements': 0,
            'successful_enhancements': 0,
            'average_confidence': 0.0
        }
    
    async def enhance_prompt(
        self,
        original_prompt: str,
        enhancement_level: Optional[EnhancementLevel] = None,
        session_history: Optional[List[Dict[str, Any]]] = None,
        force_enhancement: bool = False
    ) -> EnhancementContext:
        """æ™ºèƒ½å¢å¼ºæç¤º"""
        
        self.stats['total_enhancements'] += 1
        
        # 1. ç¡®å®šå¢å¼ºçº§åˆ«
        if enhancement_level is None:
            enhancement_level = self.config['default_enhancement_level']
            
        # 2. åˆ†æåŸå§‹æç¤º
        prompt_type = self._analyze_prompt_type(original_prompt)
        
        # 3. æ™ºèƒ½æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
        relevant_fragments = await self._retrieve_relevant_context(
            original_prompt, session_history
        )
        
        # 4. è¯„ä¼°æ˜¯å¦éœ€è¦å¢å¼º
        if not force_enhancement and not self._should_enhance(relevant_fragments):
            return EnhancementContext(
                original_prompt=original_prompt,
                enhanced_prompt=original_prompt,
                fragments_used=[],
                enhancement_reasoning="æ— éœ€å¢å¼º - æœªæ‰¾åˆ°ç›¸å…³ä¸Šä¸‹æ–‡",
                metadata={'enhancement_level': enhancement_level.value},
                confidence_score=1.0
            )
        
        # 5. æ‰§è¡Œè®°å¿†èåˆ
        context_content = self.memory_fusion.process_memory_fusion(
            relevant_fragments, 
            self.config['max_context_tokens']
        )
        
        # 6. ç”Ÿæˆå¢å¼ºæç¤º
        enhanced_prompt = self._generate_enhanced_prompt(
            original_prompt, context_content, prompt_type, enhancement_level
        )
        
        # 7. è®¡ç®—ç½®ä¿¡åº¦
        confidence = self._calculate_confidence(relevant_fragments)
        
        # 8. ç”Ÿæˆæ¨ç†è¯´æ˜
        reasoning = self._generate_enhancement_reasoning(
            relevant_fragments, prompt_type, enhancement_level
        )
        
        # 9. æ›´æ–°ç»Ÿè®¡
        if confidence >= self.config['confidence_threshold']:
            self.stats['successful_enhancements'] += 1
            
        self._update_confidence_stats(confidence)
        
        return EnhancementContext(
            original_prompt=original_prompt,
            enhanced_prompt=enhanced_prompt,
            fragments_used=relevant_fragments,
            enhancement_reasoning=reasoning,
            metadata={
                'enhancement_level': enhancement_level.value,
                'prompt_type': prompt_type.value,
                'fragments_count': len(relevant_fragments),
                'context_tokens': len(context_content) // 4
            },
            confidence_score=confidence
        )
    
    def _analyze_prompt_type(self, prompt: str) -> PromptType:
        """åˆ†ææç¤ºç±»å‹"""
        prompt_lower = prompt.lower()
        
        # ç¼–ç¨‹å…³é”®è¯
        coding_keywords = ['ä»£ç ', 'code', 'å‡½æ•°', 'function', 'å®ç°', 'implement', 
                          'class', 'ç±»', 'æ–¹æ³•', 'method', 'algorithm', 'ç®—æ³•']
        
        # è°ƒè¯•å…³é”®è¯
        debug_keywords = ['è°ƒè¯•', 'debug', 'é”™è¯¯', 'error', 'bug', 'ä¿®å¤', 'fix', 
                         'é—®é¢˜', 'issue', 'ä¸å·¥ä½œ', 'not working']
        
        # è§£é‡Šå…³é”®è¯
        explain_keywords = ['è§£é‡Š', 'explain', 'æ˜¯ä»€ä¹ˆ', 'what is', 'åŸç†', 'principle',
                           'å¦‚ä½•', 'how', 'ä¸ºä»€ä¹ˆ', 'why']
        
        # åˆ†æå…³é”®è¯
        analysis_keywords = ['åˆ†æ', 'analyze', 'è¯„ä¼°', 'evaluate', 'æ¯”è¾ƒ', 'compare',
                           'ä¼˜åŒ–', 'optimize', 'æ€§èƒ½', 'performance']
        
        if any(kw in prompt_lower for kw in debug_keywords):
            return PromptType.DEBUGGING
        elif any(kw in prompt_lower for kw in coding_keywords):
            return PromptType.CODING
        elif any(kw in prompt_lower for kw in analysis_keywords):
            return PromptType.ANALYSIS
        elif any(kw in prompt_lower for kw in explain_keywords):
            return PromptType.EXPLANATION
        else:
            return PromptType.GENERAL
    
    async def _retrieve_relevant_context(
        self,
        prompt: str,
        session_history: Optional[List[Dict[str, Any]]]
    ) -> List[RetrievalResult]:
        """æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡"""
        try:
            results = await self.retrieval_engine.intelligent_retrieve(
                query=prompt,
                strategy=RetrievalStrategy.HYBRID_ADVANCED,
                session_history=session_history,
                max_results=5
            )
            
            # è¿‡æ»¤ä½ç›¸å…³æ€§ç»“æœ
            filtered_results = [
                r for r in results 
                if r.final_score >= self.config['min_relevance_threshold']
            ]
            
            return filtered_results
            
        except Exception as e:
            logger.error(f"æ£€ç´¢ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return []
    
    def _should_enhance(self, fragments: List[RetrievalResult]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å¢å¼º"""
        if not fragments:
            return False
            
        # æ£€æŸ¥æ˜¯å¦æœ‰é«˜è´¨é‡ç‰‡æ®µ
        high_quality_count = sum(
            1 for f in fragments 
            if f.final_score >= self.config['confidence_threshold']
        )
        
        return high_quality_count > 0
    
    def _generate_enhanced_prompt(
        self,
        original_prompt: str,
        context_content: str,
        prompt_type: PromptType,
        enhancement_level: EnhancementLevel
    ) -> str:
        """ç”Ÿæˆå¢å¼ºæç¤º"""
        
        if enhancement_level == EnhancementLevel.MINIMAL and context_content:
            return f"{context_content}\n\n---\n\n{original_prompt}"
        
        return self.adaptive_generator.generate_adaptive_prompt(
            original_prompt, context_content, prompt_type, enhancement_level
        )
    
    def _calculate_confidence(self, fragments: List[RetrievalResult]) -> float:
        """è®¡ç®—å¢å¼ºç½®ä¿¡åº¦"""
        if not fragments:
            return 0.0
            
        # åŸºäºç‰‡æ®µè´¨é‡è®¡ç®—ç½®ä¿¡åº¦
        scores = [f.final_score for f in fragments]
        
        # ä½¿ç”¨åŠ æƒå¹³å‡ï¼Œæƒé‡é€’å‡
        weights = [1.0 / (i + 1) for i in range(len(scores))]
        weighted_sum = sum(score * weight for score, weight in zip(scores, weights))
        weight_sum = sum(weights)
        
        base_confidence = weighted_sum / weight_sum if weight_sum > 0 else 0.0
        
        # è€ƒè™‘ç‰‡æ®µæ•°é‡çš„å½±å“
        quantity_factor = min(len(fragments) / 2.0, 1.0)  # 2ä¸ªç‰‡æ®µä¸ºæœ€ä½³
        
        # å¦‚æœæœ‰é«˜è´¨é‡ç‰‡æ®µï¼Œæå‡ç½®ä¿¡åº¦
        high_quality_count = sum(1 for score in scores if score >= 0.7)
        if high_quality_count > 0:
            base_confidence = min(base_confidence * 1.2, 1.0)
        
        return base_confidence * quantity_factor
    
    def _generate_enhancement_reasoning(
        self,
        fragments: List[RetrievalResult],
        prompt_type: PromptType,
        enhancement_level: EnhancementLevel
    ) -> str:
        """ç”Ÿæˆå¢å¼ºæ¨ç†è¯´æ˜"""
        if not fragments:
            return "æ— ç›¸å…³è®°å¿†ç‰‡æ®µ"
            
        parts = []
        parts.append(f"ç±»å‹: {prompt_type.value}")
        parts.append(f"çº§åˆ«: {enhancement_level.value}")
        parts.append(f"ç‰‡æ®µæ•°: {len(fragments)}")
        
        # æ·»åŠ ç‰‡æ®µè´¨é‡æè¿°
        high_quality = sum(1 for f in fragments if f.final_score >= 0.7)
        medium_quality = sum(1 for f in fragments if 0.4 <= f.final_score < 0.7)
        
        if high_quality > 0:
            parts.append(f"é«˜è´¨é‡ç‰‡æ®µ: {high_quality}")
        if medium_quality > 0:
            parts.append(f"ä¸­ç­‰è´¨é‡ç‰‡æ®µ: {medium_quality}")
            
        return " | ".join(parts)
    
    def _update_confidence_stats(self, confidence: float):
        """æ›´æ–°ç½®ä¿¡åº¦ç»Ÿè®¡"""
        total = self.stats['total_enhancements']
        current_avg = self.stats['average_confidence']
        
        # å¢é‡æ›´æ–°å¹³å‡å€¼
        self.stats['average_confidence'] = (
            (current_avg * (total - 1) + confidence) / total
        )
    
    def get_enhancement_stats(self) -> Dict[str, Any]:
        """è·å–å¢å¼ºç»Ÿè®¡ä¿¡æ¯"""
        total = self.stats['total_enhancements']
        successful = self.stats['successful_enhancements']
        
        return {
            'total_enhancements': total,
            'successful_enhancements': successful,
            'success_rate': successful / total if total > 0 else 0.0,
            'average_confidence': self.stats['average_confidence'],
            'config': self.config.copy()
        }
    
    def update_config(self, **kwargs):
        """æ›´æ–°é…ç½®"""
        for key, value in kwargs.items():
            if key in self.config:
                self.config[key] = value
                logger.info(f"é…ç½®å·²æ›´æ–°: {key} = {value}")


# å·¥å‚å‡½æ•°
def create_prompt_enhancer(retrieval_engine: IntelligentRetrievalEngine, prompts_dir: Path) -> IntelligentPromptEnhancer:
    """åˆ›å»ºæç¤ºå¢å¼ºå™¨å®ä¾‹"""
    return IntelligentPromptEnhancer(retrieval_engine, prompts_dir)