#!/usr/bin/env python3
"""
ğŸš€ Sage MCP V3 é˜¶æ®µ2å®Œæ•´æµ‹è¯•å¥—ä»¶
ä¸–ç•Œçº§æ™ºèƒ½ä¸Šä¸‹æ–‡æ£€ç´¢å’Œæç¤ºå¢å¼ºç³»ç»Ÿæµ‹è¯•
"""

import unittest
import sys
import asyncio
import os
import json
import tempfile
from pathlib import Path
from unittest.mock import Mock, patch, AsyncMock
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# è®¾ç½®æµ‹è¯•ç¯å¢ƒ
os.environ['SAGE_DEBUG'] = '1'
os.environ['ORIGINAL_CLAUDE_PATH'] = 'python3 /Volumes/1T HDD/Sage/mock_claude.py'

# å¯¼å…¥æµ‹è¯•æ¨¡å—
from intelligent_retrieval import (
    IntelligentRetrievalEngine,
    AdvancedSemanticAnalyzer, 
    TemporalScoringEngine,
    HybridScoringAlgorithm,
    QueryType,
    RetrievalStrategy,
    RetrievalResult
)

from prompt_enhancer import (
    IntelligentPromptEnhancer,
    MemoryFusionProcessor,
    AdaptivePromptGenerator,
    EnhancementLevel,
    PromptType
)

from memory_interface import MemoryProviderV1, NullMemoryProvider


class TestAdvancedSemanticAnalyzer(unittest.TestCase):
    """æµ‹è¯•é«˜çº§è¯­ä¹‰åˆ†æå™¨"""
    
    def setUp(self):
        self.analyzer = AdvancedSemanticAnalyzer()
    
    def test_query_type_identification(self):
        """æµ‹è¯•æŸ¥è¯¢ç±»å‹è¯†åˆ«"""
        # æŠ€æœ¯æŸ¥è¯¢
        tech_query = "å¦‚ä½•å®ç°Pythonå‡½æ•°çš„ç¼“å­˜è£…é¥°å™¨ï¼Ÿ"
        context = self.analyzer.analyze_query(tech_query)
        self.assertEqual(context.query_type, QueryType.TECHNICAL)
        
        # è¯Šæ–­æŸ¥è¯¢
        diag_query = "æˆ‘çš„ä»£ç æŠ¥é”™äº†ï¼Œæ˜¾ç¤ºKeyError"
        context = self.analyzer.analyze_query(diag_query)
        self.assertEqual(context.query_type, QueryType.DIAGNOSTIC)
        
        # æµç¨‹æŸ¥è¯¢
        proc_query = "å¦‚ä½•ä¸€æ­¥æ­¥éƒ¨ç½²Djangoåº”ç”¨ï¼Ÿ"
        context = self.analyzer.analyze_query(proc_query)
        self.assertEqual(context.query_type, QueryType.PROCEDURAL)
    
    def test_technical_keyword_extraction(self):
        """æµ‹è¯•æŠ€æœ¯å…³é”®è¯æå–"""
        query = "å®ç°ä¸€ä¸ªRedisç¼“å­˜çš„ç”¨æˆ·è®¤è¯ç³»ç»Ÿï¼Œä½¿ç”¨JWT token"
        context = self.analyzer.analyze_query(query)
        
        # åº”è¯¥æå–åˆ°æŠ€æœ¯å…³é”®è¯
        keywords = context.technical_keywords
        self.assertTrue(any('token' in kw.lower() for kw in keywords) or 'JWT' in query)
        
    def test_emotion_analysis(self):
        """æµ‹è¯•æƒ…æ„Ÿåˆ†æ"""
        urgent_query = "ç´§æ€¥ï¼ç”Ÿäº§ç¯å¢ƒæ•°æ®åº“è¿æ¥å¤±è´¥"
        context = self.analyzer.analyze_query(urgent_query)
        self.assertEqual(context.emotional_tone, "urgent")
        self.assertGreaterEqual(context.urgency_level, 4)
        
        confused_query = "æˆ‘ä¸æ‡‚ä¸ºä»€ä¹ˆè¿™ä¸ªç®—æ³•è¿™ä¹ˆæ…¢"
        context = self.analyzer.analyze_query(confused_query)
        self.assertEqual(context.emotional_tone, "confused")


class TestTemporalScoringEngine(unittest.TestCase):
    """æµ‹è¯•æ—¶é—´è¡°å‡è¯„åˆ†å¼•æ“"""
    
    def setUp(self):
        self.engine = TemporalScoringEngine()
        self.current_time = datetime.now()
    
    def test_recent_content_boost(self):
        """æµ‹è¯•è¿‘æœŸå†…å®¹å¢å¼º"""
        # 1å°æ—¶å‰çš„å†…å®¹
        recent_time = self.current_time - timedelta(hours=1)
        score = self.engine.calculate_temporal_score(recent_time, self.current_time)
        
        # 3å¤©å‰çš„å†…å®¹ï¼ˆç¡®ä¿æ˜¾è‘—å·®å¼‚ï¼‰
        old_time = self.current_time - timedelta(days=3)
        old_score = self.engine.calculate_temporal_score(old_time, self.current_time)
        
        # è¿‘æœŸå†…å®¹åº”è¯¥å¾—åˆ†æ›´é«˜
        self.assertGreater(score, old_score)
    
    def test_urgency_influence(self):
        """æµ‹è¯•ç´§æ€¥åº¦å½±å“"""
        from intelligent_retrieval import QueryContext
        
        # é«˜ç´§æ€¥åº¦æŸ¥è¯¢
        urgent_context = QueryContext(
            query="ç´§æ€¥é—®é¢˜",
            query_type=QueryType.DIAGNOSTIC,
            urgency_level=5
        )
        
        test_time = self.current_time - timedelta(hours=6)
        urgent_score = self.engine.calculate_temporal_score(
            test_time, self.current_time, urgent_context
        )
        
        # æ™®é€šæŸ¥è¯¢
        normal_context = QueryContext(
            query="æ™®é€šé—®é¢˜",
            query_type=QueryType.CONCEPTUAL,
            urgency_level=1
        )
        
        normal_score = self.engine.calculate_temporal_score(
            test_time, self.current_time, normal_context
        )
        
        # ç´§æ€¥æŸ¥è¯¢åº”è¯¥å¯¹æ—¶é—´æ›´æ•æ„Ÿ
        self.assertGreaterEqual(urgent_score, normal_score)


class TestMemoryFusionProcessor(unittest.TestCase):
    """æµ‹è¯•è®°å¿†èåˆå¤„ç†å™¨"""
    
    def setUp(self):
        # åˆ›å»ºä¸´æ—¶promptsç›®å½•
        self.temp_dir = Path(tempfile.mkdtemp())
        self.prompts_dir = self.temp_dir / "prompts"
        self.prompts_dir.mkdir()
        
        # åˆ›å»ºæµ‹è¯•æ¨¡æ¿æ–‡ä»¶
        template_content = """
You are a Memory Fusion Assistant.

**Fragments**
---
{retrieved_passages}
---
"""
        template_file = self.prompts_dir / "memory_fusion_prompt_programming.txt"
        with open(template_file, 'w', encoding='utf-8') as f:
            f.write(template_content)
            
        self.processor = MemoryFusionProcessor(self.prompts_dir)
    
    def tearDown(self):
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def test_template_loading(self):
        """æµ‹è¯•æ¨¡æ¿åŠ è½½"""
        self.assertIsNotNone(self.processor.user_template)
        self.assertIn("{retrieved_passages}", self.processor.user_template)
    
    def test_memory_fusion(self):
        """æµ‹è¯•è®°å¿†èåˆ"""
        # åˆ›å»ºæµ‹è¯•ç‰‡æ®µ
        fragments = [
            RetrievalResult(
                content="è¿™æ˜¯ä¸€ä¸ªPythonå‡½æ•°ç¤ºä¾‹",
                role="assistant",
                similarity_score=0.8,
                temporal_score=0.7,
                context_score=0.6,
                final_score=0.7,
                reasoning="é«˜ç›¸ä¼¼åº¦"
            ),
            RetrievalResult(
                content="ç”¨æˆ·çš„ç›¸å…³é—®é¢˜",
                role="user", 
                similarity_score=0.6,
                temporal_score=0.8,
                context_score=0.5,
                final_score=0.6,
                reasoning="æ—¶æ•ˆæ€§å¼º"
            )
        ]
        
        result = self.processor.process_memory_fusion(fragments)
        
        # éªŒè¯èåˆç»“æœ
        self.assertIn("Pythonå‡½æ•°ç¤ºä¾‹", result)
        self.assertIn("ç”¨æˆ·çš„ç›¸å…³é—®é¢˜", result)
        self.assertIn("<fragment_01>", result)
        self.assertIn("<fragment_02>", result)


class MockMemoryProvider:
    """æ¨¡æ‹Ÿè®°å¿†æä¾›è€…"""
    
    def __init__(self):
        self.test_data = [
            {
                'content': 'Pythonä¸­å¦‚ä½•ä½¿ç”¨è£…é¥°å™¨ï¼Ÿ',
                'role': 'user',
                'score': 0.8,
                'metadata': {'timestamp': datetime.now().isoformat()}
            },
            {
                'content': 'è£…é¥°å™¨æ˜¯Pythonä¸­çš„ä¸€ç§è®¾è®¡æ¨¡å¼...',
                'role': 'assistant',
                'score': 0.7,
                'metadata': {'timestamp': (datetime.now() - timedelta(hours=1)).isoformat()}
            },
            {
                'content': 'å¦‚ä½•è°ƒè¯•Pythonæ€§èƒ½é—®é¢˜ï¼Ÿ',
                'role': 'user',
                'score': 0.6,
                'metadata': {'timestamp': (datetime.now() - timedelta(days=1)).isoformat()}
            }
        ]
    
    def search_memory(self, query, n=5):
        """æ¨¡æ‹Ÿæœç´¢è®°å¿†"""
        from memory_interface import MemorySearchResult
        results = []
        
        for item in self.test_data[:n]:
            results.append(MemorySearchResult(
                content=item['content'],
                role=item['role'],
                score=item['score'],
                metadata=item['metadata']
            ))
        
        return results


class TestIntelligentRetrievalEngine(unittest.TestCase):
    """æµ‹è¯•æ™ºèƒ½æ£€ç´¢å¼•æ“"""
    
    def setUp(self):
        self.memory_provider = MockMemoryProvider()
        self.engine = IntelligentRetrievalEngine(self.memory_provider)
    
    def test_engine_initialization(self):
        """æµ‹è¯•å¼•æ“åˆå§‹åŒ–"""
        self.assertIsNotNone(self.engine.semantic_analyzer)
        self.assertIsNotNone(self.engine.scoring_algorithm)
        self.assertEqual(self.engine.config['max_results'], 10)
    
    async def test_intelligent_retrieve(self):
        """æµ‹è¯•æ™ºèƒ½æ£€ç´¢"""
        query = "Pythonè£…é¥°å™¨çš„ä½¿ç”¨æ–¹æ³•"
        
        results = await self.engine.intelligent_retrieve(
            query=query,
            strategy=RetrievalStrategy.HYBRID_ADVANCED,
            max_results=3
        )
        
        # éªŒè¯ç»“æœ
        self.assertIsInstance(results, list)
        self.assertLessEqual(len(results), 3)
        
        if results:
            # éªŒè¯ç»“æœç»“æ„
            result = results[0]
            self.assertIsInstance(result, RetrievalResult)
            self.assertIsInstance(result.final_score, float)
            self.assertIsInstance(result.reasoning, str)
            
            # éªŒè¯å¾—åˆ†æ’åº
            for i in range(1, len(results)):
                self.assertGreaterEqual(results[i-1].final_score, results[i].final_score)
    
    def test_diversity_filter(self):
        """æµ‹è¯•å¤šæ ·æ€§è¿‡æ»¤"""
        # åˆ›å»ºç›¸ä¼¼çš„æµ‹è¯•ç»“æœ
        similar_results = [
            RetrievalResult(
                content="Pythonè£…é¥°å™¨ç¤ºä¾‹1",
                role="assistant",
                similarity_score=0.9,
                temporal_score=0.8,
                context_score=0.7,
                final_score=0.85,
                reasoning="é«˜ç›¸ä¼¼åº¦"
            ),
            RetrievalResult(
                content="Pythonè£…é¥°å™¨ç¤ºä¾‹2",
                role="assistant", 
                similarity_score=0.8,
                temporal_score=0.7,
                context_score=0.6,
                final_score=0.75,
                reasoning="ä¸­ç­‰ç›¸ä¼¼åº¦"
            ),
            RetrievalResult(
                content="Djangoæ¨¡å‹ä½¿ç”¨æ–¹æ³•",
                role="assistant",
                similarity_score=0.7,
                temporal_score=0.6,
                context_score=0.5,
                final_score=0.65,
                reasoning="ä¸åŒä¸»é¢˜"
            )
        ]
        
        filtered = self.engine._apply_diversity_filter(similar_results, 2)
        
        # åº”è¯¥é€‰æ‹©å¤šæ ·æ€§æ›´å¥½çš„ç»„åˆ
        self.assertEqual(len(filtered), 2)
        
        # ç¬¬ä¸€ä¸ªåº”è¯¥æ˜¯å¾—åˆ†æœ€é«˜çš„
        self.assertEqual(filtered[0].final_score, 0.85)


class TestPromptEnhancer(unittest.TestCase):
    """æµ‹è¯•æç¤ºå¢å¼ºå™¨"""
    
    def setUp(self):
        # åˆ›å»ºæ¨¡æ‹Ÿæ£€ç´¢å¼•æ“
        self.mock_engine = Mock()
        self.mock_engine.intelligent_retrieve = AsyncMock()
        
        # åˆ›å»ºä¸´æ—¶promptsç›®å½•
        self.temp_dir = Path(tempfile.mkdtemp())
        self.prompts_dir = self.temp_dir / "prompts"
        self.prompts_dir.mkdir()
        
        # åˆ›å»ºæµ‹è¯•æ¨¡æ¿
        template_file = self.prompts_dir / "memory_fusion_prompt_programming.txt"
        with open(template_file, 'w') as f:
            f.write("Test template: {retrieved_passages}")
            
        self.enhancer = IntelligentPromptEnhancer(self.mock_engine, self.prompts_dir)
    
    def tearDown(self):
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def test_prompt_type_analysis(self):
        """æµ‹è¯•æç¤ºç±»å‹åˆ†æ"""
        # ç¼–ç¨‹ç±»å‹
        coding_prompt = "è¯·å¸®æˆ‘å®ç°ä¸€ä¸ªPythonå‡½æ•°"
        prompt_type = self.enhancer._analyze_prompt_type(coding_prompt)
        self.assertEqual(prompt_type, PromptType.CODING)
        
        # è°ƒè¯•ç±»å‹
        debug_prompt = "æˆ‘çš„ä»£ç æœ‰bugï¼Œæ€ä¹ˆä¿®å¤ï¼Ÿ"
        prompt_type = self.enhancer._analyze_prompt_type(debug_prompt)
        self.assertEqual(prompt_type, PromptType.DEBUGGING)
        
        # è§£é‡Šç±»å‹
        explain_prompt = "è¯·è§£é‡Šä»€ä¹ˆæ˜¯RESTful API"
        prompt_type = self.enhancer._analyze_prompt_type(explain_prompt)
        self.assertEqual(prompt_type, PromptType.EXPLANATION)
    
    async def test_enhance_prompt_with_results(self):
        """æµ‹è¯•å¸¦ç»“æœçš„æç¤ºå¢å¼º"""
        # è®¾ç½®æ¨¡æ‹Ÿè¿”å›
        mock_results = [
            RetrievalResult(
                content="ç›¸å…³çš„æŠ€æœ¯è§£ç­”",
                role="assistant",
                similarity_score=0.8,
                temporal_score=0.7,
                context_score=0.6,
                final_score=0.75,
                reasoning="é«˜åº¦ç›¸å…³"
            )
        ]
        self.mock_engine.intelligent_retrieve.return_value = mock_results
        
        # æ‰§è¡Œå¢å¼º
        context = await self.enhancer.enhance_prompt(
            "å¦‚ä½•ä¼˜åŒ–Pythonä»£ç æ€§èƒ½ï¼Ÿ",
            EnhancementLevel.STANDARD
        )
        
        # éªŒè¯ç»“æœ
        self.assertIsNotNone(context.enhanced_prompt)
        self.assertEqual(len(context.fragments_used), 1)
        self.assertGreater(context.confidence_score, 0)
        self.assertIn("æŠ€æœ¯è§£ç­”", context.enhanced_prompt)
    
    async def test_enhance_prompt_no_results(self):
        """æµ‹è¯•æ— ç»“æœçš„æç¤ºå¢å¼º"""
        # è®¾ç½®ç©ºè¿”å›
        self.mock_engine.intelligent_retrieve.return_value = []
        
        # æ‰§è¡Œå¢å¼º
        context = await self.enhancer.enhance_prompt(
            "è¿™æ˜¯ä¸€ä¸ªå…¨æ–°çš„é—®é¢˜",
            EnhancementLevel.ADAPTIVE
        )
        
        # éªŒè¯ç»“æœ
        self.assertEqual(context.enhanced_prompt, "è¿™æ˜¯ä¸€ä¸ªå…¨æ–°çš„é—®é¢˜")
        self.assertEqual(len(context.fragments_used), 0)
        self.assertEqual(context.confidence_score, 1.0)  # æ— éœ€å¢å¼ºæ—¶ç½®ä¿¡åº¦ä¸º1
    
    def test_confidence_calculation(self):
        """æµ‹è¯•ç½®ä¿¡åº¦è®¡ç®—"""
        # é«˜è´¨é‡ç‰‡æ®µ
        high_quality = [
            RetrievalResult("content1", "user", 0.9, 0.8, 0.7, 0.85, {}, "excellent"),
            RetrievalResult("content2", "assistant", 0.8, 0.7, 0.6, 0.75, {}, "good")
        ]
        
        confidence = self.enhancer._calculate_confidence(high_quality)
        self.assertGreater(confidence, 0.7)
        
        # ä½è´¨é‡ç‰‡æ®µ
        low_quality = [
            RetrievalResult("content3", "user", 0.3, 0.2, 0.1, 0.25, {}, "poor")
        ]
        
        low_confidence = self.enhancer._calculate_confidence(low_quality)
        self.assertLess(low_confidence, 0.5)
    
    def test_enhancement_stats(self):
        """æµ‹è¯•å¢å¼ºç»Ÿè®¡"""
        stats = self.enhancer.get_enhancement_stats()
        
        # éªŒè¯ç»Ÿè®¡ç»“æ„
        self.assertIn('total_enhancements', stats)
        self.assertIn('successful_enhancements', stats)
        self.assertIn('success_rate', stats)
        self.assertIn('average_confidence', stats)
        self.assertIn('config', stats)


class TestIntegration(unittest.TestCase):
    """é›†æˆæµ‹è¯•"""
    
    def setUp(self):
        # ä½¿ç”¨ç©ºè®°å¿†æä¾›è€…é¿å…çœŸå®æ•°æ®åº“ä¾èµ–
        self.memory_provider = NullMemoryProvider()
        self.temp_dir = Path(tempfile.mkdtemp())
    
    def tearDown(self):
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    async def test_end_to_end_enhancement(self):
        """ç«¯åˆ°ç«¯å¢å¼ºæµ‹è¯•"""
        # åˆ›å»ºå¸¦æ¨¡æ‹Ÿæ•°æ®çš„è®°å¿†æä¾›è€…
        memory_provider = MockMemoryProvider()
        
        # åˆ›å»ºæ£€ç´¢å¼•æ“
        engine = IntelligentRetrievalEngine(memory_provider)
        
        # åˆ›å»ºpromptsç›®å½•å’Œæ¨¡æ¿
        prompts_dir = self.temp_dir / "prompts"
        prompts_dir.mkdir()
        template_file = prompts_dir / "memory_fusion_prompt_programming.txt"
        with open(template_file, 'w') as f:
            f.write("Context: {retrieved_passages}\n\nQuery: ")
        
        # åˆ›å»ºå¢å¼ºå™¨
        enhancer = IntelligentPromptEnhancer(engine, prompts_dir)
        
        # æ‰§è¡Œå®Œæ•´æµç¨‹
        query = "å¦‚ä½•ä½¿ç”¨Pythonè£…é¥°å™¨ï¼Ÿ"
        context = await enhancer.enhance_prompt(query, EnhancementLevel.COMPREHENSIVE)
        
        # éªŒè¯å®Œæ•´æµç¨‹
        self.assertIsNotNone(context.enhanced_prompt)
        self.assertIn(query, context.enhanced_prompt or "")


def run_async_test(test_method):
    """è¿è¡Œå¼‚æ­¥æµ‹è¯•çš„è¾…åŠ©å‡½æ•°"""
    def wrapper(self):
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(test_method(self))
        finally:
            loop.close()
    return wrapper


# ä¸ºå¼‚æ­¥æµ‹è¯•æ–¹æ³•æ·»åŠ è£…é¥°å™¨
TestIntelligentRetrievalEngine.test_intelligent_retrieve = run_async_test(
    TestIntelligentRetrievalEngine.test_intelligent_retrieve
)
TestPromptEnhancer.test_enhance_prompt_with_results = run_async_test(
    TestPromptEnhancer.test_enhance_prompt_with_results
)
TestPromptEnhancer.test_enhance_prompt_no_results = run_async_test(
    TestPromptEnhancer.test_enhance_prompt_no_results
)
TestIntegration.test_end_to_end_enhancement = run_async_test(
    TestIntegration.test_end_to_end_enhancement
)


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ Sage MCP V3 é˜¶æ®µ2å®Œæ•´æµ‹è¯•")
    print("=" * 80)
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_classes = [
        TestAdvancedSemanticAnalyzer,
        TestTemporalScoringEngine, 
        TestMemoryFusionProcessor,
        TestIntelligentRetrievalEngine,
        TestPromptEnhancer,
        TestIntegration
    ]
    
    total_tests = 0
    total_failures = 0
    total_errors = 0
    
    for test_class in test_classes:
        print(f"\nğŸ§ª è¿è¡Œ {test_class.__name__}")
        print("-" * 60)
        
        suite = unittest.TestLoader().loadTestsFromTestCase(test_class)
        runner = unittest.TextTestRunner(verbosity=2)
        result = runner.run(suite)
        
        total_tests += result.testsRun
        total_failures += len(result.failures)
        total_errors += len(result.errors)
    
    print("\n" + "=" * 80)
    print("é˜¶æ®µ2å®Œæ•´æµ‹è¯•æŠ¥å‘Š")
    print("=" * 80)
    print(f"è¿è¡Œæµ‹è¯•æ•°: {total_tests}")
    print(f"æˆåŠŸ: {total_tests - total_failures - total_errors}")
    print(f"å¤±è´¥: {total_failures}")
    print(f"é”™è¯¯: {total_errors}")
    
    if total_failures == 0 and total_errors == 0:
        print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼é˜¶æ®µ2æ™ºèƒ½ç³»ç»Ÿè¿è¡Œå®Œç¾ã€‚")
        print("ğŸš€ ä¸–ç•Œçº§æ™ºèƒ½ä¸Šä¸‹æ–‡æ£€ç´¢å’Œæç¤ºå¢å¼ºç³»ç»Ÿå·²å°±ç»ªï¼")
    else:
        print(f"\nâŒ å‘ç° {total_failures + total_errors} ä¸ªé—®é¢˜éœ€è¦ä¿®å¤")
    
    return total_failures + total_errors == 0


if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)