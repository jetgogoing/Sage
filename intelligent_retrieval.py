#!/usr/bin/env python3
"""
Sage MCP æ™ºèƒ½ä¸Šä¸‹æ–‡æ£€ç´¢å¼•æ“ V2.0
ğŸš€ ä¸–ç•Œçº§æ£€ç´¢ç®—æ³•å®ç°ï¼Œæ”¯æŒå¤šç»´åº¦ç›¸ä¼¼åº¦è®¡ç®—ã€æ—¶é—´è¡°å‡ã€è¯­ä¹‰ç†è§£å’ŒåŠ¨æ€æƒé‡è°ƒæ•´
"""

import math
import asyncio
import hashlib
import statistics
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
import logging
import json
import re
from collections import defaultdict, Counter

logger = logging.getLogger('SageIntelligentRetrieval')

# Import Qwen Reranker if available
try:
    from reranker_qwen import HybridReranker, RerankingMode
    RERANKER_AVAILABLE = True
except ImportError:
    RERANKER_AVAILABLE = False
    logger.warning("Qwen Reranker not available, using standard retrieval only")


class QueryType(Enum):
    """æŸ¥è¯¢ç±»å‹æšä¸¾"""
    TECHNICAL = "technical"      # æŠ€æœ¯æŸ¥è¯¢ï¼ˆä»£ç ã€è°ƒè¯•ã€å®ç°ï¼‰
    CONCEPTUAL = "conceptual"    # æ¦‚å¿µæŸ¥è¯¢ï¼ˆè§£é‡Šã€åŸç†ã€ç†è®ºï¼‰
    PROCEDURAL = "procedural"    # æµç¨‹æŸ¥è¯¢ï¼ˆå¦‚ä½•åšã€æ­¥éª¤ã€æ–¹æ³•ï¼‰
    DIAGNOSTIC = "diagnostic"    # è¯Šæ–­æŸ¥è¯¢ï¼ˆé”™è¯¯ã€é—®é¢˜ã€æ•…éšœæ’é™¤ï¼‰
    CREATIVE = "creative"        # åˆ›æ„æŸ¥è¯¢ï¼ˆè®¾è®¡ã€åˆ›æ–°ã€æ–¹æ¡ˆï¼‰
    CONVERSATIONAL = "conversational"  # å¯¹è¯å»¶ç»­ï¼ˆåŸºäºä¸Šä¸‹æ–‡ï¼‰


class RetrievalStrategy(Enum):
    """æ£€ç´¢ç­–ç•¥æšä¸¾"""
    SEMANTIC_FIRST = "semantic_first"      # è¯­ä¹‰ä¼˜å…ˆ
    TEMPORAL_WEIGHTED = "temporal_weighted"  # æ—¶é—´åŠ æƒ
    CONTEXT_AWARE = "context_aware"        # ä¸Šä¸‹æ–‡æ„ŸçŸ¥
    HYBRID_ADVANCED = "hybrid_advanced"    # æ··åˆé«˜çº§ç­–ç•¥
    ADAPTIVE = "adaptive"                  # è‡ªé€‚åº”ç­–ç•¥


@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœ"""
    content: str
    role: str
    similarity_score: float
    temporal_score: float
    context_score: float
    final_score: float
    metadata: Dict[str, Any] = field(default_factory=dict)
    reasoning: str = ""  # é€‰æ‹©åŸå› 


@dataclass
class QueryContext:
    """æŸ¥è¯¢ä¸Šä¸‹æ–‡"""
    query: str
    query_type: QueryType
    session_history: List[Dict[str, Any]] = field(default_factory=list)
    user_intent: Optional[str] = None
    technical_keywords: List[str] = field(default_factory=list)
    emotional_tone: str = "neutral"
    urgency_level: int = 1  # 1-5çº§ç´§æ€¥ç¨‹åº¦


class AdvancedSemanticAnalyzer:
    """é«˜çº§è¯­ä¹‰åˆ†æå™¨"""
    
    def __init__(self):
        # æŠ€æœ¯å…³é”®è¯åº“ï¼ˆå¯æ‰©å±•ï¼‰
        self.technical_patterns = {
            'programming': ['å‡½æ•°', 'function', 'class', 'ç±»', 'æ–¹æ³•', 'method', 'å˜é‡', 'variable', 
                          'API', 'algorithm', 'ç®—æ³•', 'bug', 'debug', 'è°ƒè¯•', 'error', 'é”™è¯¯'],
            'database': ['æ•°æ®åº“', 'database', 'SQL', 'query', 'æŸ¥è¯¢', 'table', 'è¡¨', 'index', 'ç´¢å¼•'],
            'system': ['ç³»ç»Ÿ', 'system', 'æ¶æ„', 'architecture', 'æ€§èƒ½', 'performance', 'ä¼˜åŒ–', 'optimization'],
            'network': ['ç½‘ç»œ', 'network', 'HTTP', 'API', 'æ¥å£', 'interface', 'åè®®', 'protocol'],
            'data': ['æ•°æ®', 'data', 'åˆ†æ', 'analysis', 'ç»Ÿè®¡', 'statistics', 'æ¨¡å‹', 'model']
        }
        
        # æƒ…æ„Ÿè¯å…¸
        self.emotion_patterns = {
            'urgent': ['ç´§æ€¥', 'urgent', 'æ€¥', 'ç«‹å³', 'immediately', 'é©¬ä¸Š', 'ASAP'],
            'confused': ['ä¸æ‡‚', 'confused', 'å›°æƒ‘', 'ä¸ç†è§£', "don't understand", 'æä¸æ¸…æ¥š'],
            'frustrated': ['çƒ¦èº', 'frustrated', 'å¤´ç–¼', 'éº»çƒ¦', 'trouble', 'é—®é¢˜'],
            'curious': ['å¥½å¥‡', 'curious', 'æƒ³çŸ¥é“', 'wonder', 'äº†è§£', 'å­¦ä¹ ', 'learn']
        }
        
        # æ„å›¾æ¨¡å¼
        self.intent_patterns = {
            'implementation': ['å¦‚ä½•å®ç°', 'how to implement', 'æ€ä¹ˆåš', 'å®ç°æ–¹æ³•', 'ä»£ç ç¤ºä¾‹'],
            'explanation': ['æ˜¯ä»€ä¹ˆ', 'what is', 'è§£é‡Š', 'explain', 'åŸç†', 'principle'],
            'troubleshooting': ['ä¸å·¥ä½œ', 'not working', 'é”™è¯¯', 'error', 'å¤±è´¥', 'failed', 'é—®é¢˜'],
            'comparison': ['æ¯”è¾ƒ', 'compare', 'åŒºåˆ«', 'difference', 'é€‰æ‹©', 'choose', 'vs'],
            'optimization': ['ä¼˜åŒ–', 'optimize', 'æ”¹è¿›', 'improve', 'æå‡', 'enhance', 'æ€§èƒ½']
        }
    
    def analyze_query(self, query: str) -> QueryContext:
        """æ·±åº¦åˆ†ææŸ¥è¯¢æ„å›¾å’Œä¸Šä¸‹æ–‡"""
        query_lower = query.lower()
        
        # 1. è¯†åˆ«æŸ¥è¯¢ç±»å‹
        query_type = self._identify_query_type(query_lower)
        
        # 2. æå–æŠ€æœ¯å…³é”®è¯
        tech_keywords = self._extract_technical_keywords(query)
        
        # 3. åˆ†ææƒ…æ„ŸåŸºè°ƒ
        emotion = self._analyze_emotion(query_lower)
        
        # 4. æ¨æ–­ç”¨æˆ·æ„å›¾
        intent = self._infer_intent(query_lower)
        
        # 5. è¯„ä¼°ç´§æ€¥ç¨‹åº¦
        urgency = self._assess_urgency(query_lower, emotion)
        
        return QueryContext(
            query=query,
            query_type=query_type,
            technical_keywords=tech_keywords,
            emotional_tone=emotion,
            user_intent=intent,
            urgency_level=urgency
        )
    
    def _identify_query_type(self, query_lower: str) -> QueryType:
        """è¯†åˆ«æŸ¥è¯¢ç±»å‹"""
        # è¯Šæ–­æŸ¥è¯¢æ¨¡å¼ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        diagnostic_patterns = ['é”™è¯¯', 'error', 'bug', 'ä¸å·¥ä½œ', 'å¤±è´¥', 'é—®é¢˜', 'æŠ¥é”™', 'keyerror', 'exception']
        if any(pattern in query_lower for pattern in diagnostic_patterns):
            return QueryType.DIAGNOSTIC
            
        # æŠ€æœ¯æŸ¥è¯¢æ¨¡å¼
        tech_patterns = ['ä»£ç ', 'code', 'å‡½æ•°', 'function', 'class', 'å®ç°', 'implement', 'å¼€å‘']
        if any(pattern in query_lower for pattern in tech_patterns):
            return QueryType.TECHNICAL
            
        # æµç¨‹æŸ¥è¯¢æ¨¡å¼
        proc_patterns = ['å¦‚ä½•', 'how to', 'æ­¥éª¤', 'step', 'æ–¹æ³•', 'method', 'ä¸€æ­¥æ­¥']
        if any(pattern in query_lower for pattern in proc_patterns):
            return QueryType.PROCEDURAL
            
        # æ¦‚å¿µæŸ¥è¯¢æ¨¡å¼
        concept_patterns = ['æ˜¯ä»€ä¹ˆ', 'what is', 'è§£é‡Š', 'explain', 'åŸç†', 'principle']
        if any(pattern in query_lower for pattern in concept_patterns):
            return QueryType.CONCEPTUAL
            
        # å¯¹è¯å»¶ç»­æ¨¡å¼
        conv_patterns = ['ç»§ç»­', 'continue', 'ç„¶å', 'then', 'æ¥ä¸‹æ¥']
        if any(pattern in query_lower for pattern in conv_patterns):
            return QueryType.CONVERSATIONAL
            
        return QueryType.CONCEPTUAL  # é»˜è®¤
    
    def _extract_technical_keywords(self, query: str) -> List[str]:
        """æå–æŠ€æœ¯å…³é”®è¯"""
        keywords = []
        query_lower = query.lower()
        
        for category, patterns in self.technical_patterns.items():
            for pattern in patterns:
                if pattern.lower() in query_lower:
                    keywords.append(pattern)
                    
        # æå–ä»£ç æ ‡è¯†ç¬¦ï¼ˆé©¼å³°å‘½åã€ä¸‹åˆ’çº¿ç­‰ï¼‰
        code_patterns = re.findall(r'\b[a-zA-Z_][a-zA-Z0-9_]*[A-Z][a-zA-Z0-9_]*\b', query)  # é©¼å³°
        code_patterns += re.findall(r'\b[a-z_]+_[a-z_]+\b', query)  # ä¸‹åˆ’çº¿
        keywords.extend(code_patterns)
        
        return list(set(keywords))
    
    def _analyze_emotion(self, query_lower: str) -> str:
        """åˆ†ææƒ…æ„ŸåŸºè°ƒ"""
        for emotion, patterns in self.emotion_patterns.items():
            if any(pattern in query_lower for pattern in patterns):
                return emotion
        return "neutral"
    
    def _infer_intent(self, query_lower: str) -> str:
        """æ¨æ–­ç”¨æˆ·æ„å›¾"""
        for intent, patterns in self.intent_patterns.items():
            if any(pattern in query_lower for pattern in patterns):
                return intent
        return "general"
    
    def _assess_urgency(self, query_lower: str, emotion: str) -> int:
        """è¯„ä¼°ç´§æ€¥ç¨‹åº¦"""
        urgency = 1
        
        # åŸºäºæƒ…æ„Ÿè°ƒæ•´
        if emotion == "urgent":
            urgency = 5
        elif emotion == "frustrated":
            urgency = 4
        elif emotion == "confused":
            urgency = 3
            
        # åŸºäºå…³é”®è¯è°ƒæ•´
        urgent_keywords = ['ç´§æ€¥', 'urgent', 'ç«‹å³', 'critical', 'ä¸¥é‡', 'severe']
        if any(kw in query_lower for kw in urgent_keywords):
            urgency = max(urgency, 4)
            
        return min(urgency, 5)


class TemporalScoringEngine:
    """æ—¶é—´è¡°å‡è¯„åˆ†å¼•æ“"""
    
    def __init__(self):
        self.decay_params = {
            'exponential_base': 0.95,    # æŒ‡æ•°è¡°å‡åŸºæ•°
            'linear_factor': 0.1,        # çº¿æ€§è¡°å‡å› å­
            'recency_boost': 2.0,        # è¿‘æœŸå¢å¼ºå› å­
            'session_bonus': 1.5,        # ä¼šè¯å†…å¥–åŠ±
            'peak_hours': 24 * 7,        # å³°å€¼æ—¶é—´ï¼ˆå°æ—¶ï¼‰
        }
    
    def calculate_temporal_score(
        self,
        timestamp: datetime,
        current_time: Optional[datetime] = None,
        query_context: Optional[QueryContext] = None
    ) -> float:
        """è®¡ç®—æ—¶é—´ç›¸å…³æ€§å¾—åˆ†"""
        if current_time is None:
            current_time = datetime.now()
            
        hours_diff = (current_time - timestamp).total_seconds() / 3600
        
        # 1. åŸºç¡€æŒ‡æ•°è¡°å‡
        base_score = self.decay_params['exponential_base'] ** (hours_diff / 24)
        
        # 2. è¿‘æœŸå¢å¼ºï¼ˆ24å°æ—¶å†…ï¼‰
        if hours_diff <= 24:
            base_score *= self.decay_params['recency_boost']
            
        # 3. ä¼šè¯å†…å¥–åŠ±ï¼ˆ1å°æ—¶å†…ï¼‰
        if hours_diff <= 1:
            base_score *= self.decay_params['session_bonus']
        
        # 4. ç¡®ä¿æœ‰æ˜æ˜¾çš„æ—¶é—´å·®å¼‚
        if hours_diff <= 1:
            base_score = max(base_score, 0.9)
        elif hours_diff <= 24:
            base_score = max(base_score, 0.7)
            
        # 4. æ ¹æ®æŸ¥è¯¢ç´§æ€¥ç¨‹åº¦è°ƒæ•´
        if query_context and query_context.urgency_level >= 4:
            # ç´§æ€¥æŸ¥è¯¢æ›´é‡è§†è¿‘æœŸå†…å®¹
            base_score *= (1 + (5 - query_context.urgency_level) * 0.2)
            
        return min(base_score, 1.0)
    
    def calculate_session_relevance(
        self,
        content_metadata: Dict[str, Any],
        session_history: List[Dict[str, Any]]
    ) -> float:
        """è®¡ç®—ä¼šè¯ç›¸å…³æ€§"""
        if not session_history:
            return 0.0
            
        session_score = 0.0
        content_session = content_metadata.get('session_id', '')
        
        # æ£€æŸ¥æ˜¯å¦åœ¨åŒä¸€ä¼šè¯ä¸­
        for history_item in session_history:
            if history_item.get('session_id') == content_session:
                session_score += 0.3
                
        # æ£€æŸ¥ä¸»é¢˜è¿ç»­æ€§
        content_keywords = set(content_metadata.get('keywords', []))
        for history_item in session_history:
            history_keywords = set(history_item.get('keywords', []))
            overlap = len(content_keywords & history_keywords)
            if overlap > 0:
                session_score += overlap * 0.1
                
        return min(session_score, 1.0)


class HybridScoringAlgorithm:
    """æ··åˆè¯„åˆ†ç®—æ³•å¼•æ“"""
    
    def __init__(self):
        self.semantic_analyzer = AdvancedSemanticAnalyzer()
        self.temporal_engine = TemporalScoringEngine()
        
        # åŠ¨æ€æƒé‡é…ç½®ï¼ˆå¯æ ¹æ®æŸ¥è¯¢ç±»å‹è°ƒæ•´ï¼‰
        self.weight_profiles = {
            QueryType.TECHNICAL: {
                'semantic': 0.5,
                'temporal': 0.2,
                'context': 0.2,
                'keyword': 0.1
            },
            QueryType.DIAGNOSTIC: {
                'semantic': 0.4,
                'temporal': 0.3,
                'context': 0.2,
                'keyword': 0.1
            },
            QueryType.CONVERSATIONAL: {
                'semantic': 0.3,
                'temporal': 0.4,
                'context': 0.3,
                'keyword': 0.0
            },
            QueryType.CONCEPTUAL: {
                'semantic': 0.6,
                'temporal': 0.1,
                'context': 0.2,
                'keyword': 0.1
            },
            QueryType.PROCEDURAL: {
                'semantic': 0.5,
                'temporal': 0.2,
                'context': 0.2,
                'keyword': 0.1
            }
        }
    
    def calculate_comprehensive_score(
        self,
        content: Dict[str, Any],
        query_context: QueryContext,
        base_similarity: float
    ) -> Tuple[float, str]:
        """è®¡ç®—ç»¼åˆå¾—åˆ†"""
        
        # 1. è¯­ä¹‰å¾—åˆ†ï¼ˆåŸºç¡€ç›¸ä¼¼åº¦ï¼‰
        semantic_score = base_similarity
        
        # 2. æ—¶é—´å¾—åˆ†
        timestamp = content.get('timestamp')
        if timestamp:
            if isinstance(timestamp, str):
                timestamp = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            temporal_score = self.temporal_engine.calculate_temporal_score(
                timestamp, query_context=query_context
            )
        else:
            temporal_score = 0.5  # é»˜è®¤ä¸­ç­‰æ—¶é—´å¾—åˆ†
            
        # 3. ä¸Šä¸‹æ–‡å¾—åˆ†
        context_score = self._calculate_context_score(content, query_context)
        
        # 4. å…³é”®è¯åŒ¹é…å¾—åˆ†
        keyword_score = self._calculate_keyword_score(content, query_context)
        
        # 5. è·å–æƒé‡é…ç½®
        weights = self.weight_profiles.get(query_context.query_type, 
                                         self.weight_profiles[QueryType.CONCEPTUAL])
        
        # 6. è®¡ç®—æœ€ç»ˆå¾—åˆ†
        final_score = (
            semantic_score * weights['semantic'] +
            temporal_score * weights['temporal'] +
            context_score * weights['context'] +
            keyword_score * weights['keyword']
        )
        
        # 7. ç”Ÿæˆæ¨ç†è¯´æ˜
        reasoning = self._generate_reasoning(
            semantic_score, temporal_score, context_score, keyword_score, weights
        )
        
        return final_score, reasoning
    
    def _calculate_context_score(self, content: Dict[str, Any], query_context: QueryContext) -> float:
        """è®¡ç®—ä¸Šä¸‹æ–‡ç›¸å…³æ€§å¾—åˆ†"""
        context_score = 0.0
        
        # 1. ä¼šè¯è¿ç»­æ€§
        if query_context.session_history:
            session_score = self.temporal_engine.calculate_session_relevance(
                content.get('metadata', {}), query_context.session_history
            )
            context_score += session_score * 0.4
            
        # 2. è§’è‰²ä¸€è‡´æ€§ï¼ˆç”¨æˆ·é—®é¢˜ vs åŠ©æ‰‹å›ç­”ï¼‰
        content_role = content.get('role', '')
        if query_context.query_type == QueryType.CONVERSATIONAL:
            if content_role == 'assistant':  # å¯¹è¯å»¶ç»­æ›´éœ€è¦åŠ©æ‰‹çš„å›ç­”
                context_score += 0.3
        elif content_role == 'user':  # å…¶ä»–æŸ¥è¯¢æ›´å…³æ³¨ç›¸ä¼¼é—®é¢˜
            context_score += 0.2
            
        # 3. æŠ€æœ¯é¢†åŸŸåŒ¹é…
        content_keywords = content.get('metadata', {}).get('keywords', [])
        query_keywords = query_context.technical_keywords
        if content_keywords and query_keywords:
            keyword_overlap = len(set(content_keywords) & set(query_keywords))
            context_score += (keyword_overlap / max(len(query_keywords), 1)) * 0.3
            
        return min(context_score, 1.0)
    
    def _calculate_keyword_score(self, content: Dict[str, Any], query_context: QueryContext) -> float:
        """è®¡ç®—å…³é”®è¯åŒ¹é…å¾—åˆ†"""
        if not query_context.technical_keywords:
            return 0.0
            
        content_text = content.get('content', '').lower()
        keyword_matches = 0
        
        for keyword in query_context.technical_keywords:
            if keyword.lower() in content_text:
                keyword_matches += 1
                
        return keyword_matches / len(query_context.technical_keywords)
    
    def _generate_reasoning(
        self,
        semantic: float,
        temporal: float,
        context: float,
        keyword: float,
        weights: Dict[str, float]
    ) -> str:
        """ç”Ÿæˆå¾—åˆ†æ¨ç†è¯´æ˜"""
        components = []
        
        if semantic > 0.7:
            components.append(f"é«˜è¯­ä¹‰ç›¸ä¼¼åº¦({semantic:.2f})")
        elif semantic > 0.5:
            components.append(f"ä¸­ç­‰è¯­ä¹‰ç›¸ä¼¼åº¦({semantic:.2f})")
            
        if temporal > 0.8:
            components.append("æ—¶æ•ˆæ€§å¼º")
        elif temporal > 0.5:
            components.append("æ—¶æ•ˆæ€§ä¸­ç­‰")
            
        if context > 0.6:
            components.append("ä¸Šä¸‹æ–‡ç›¸å…³")
            
        if keyword > 0.5:
            components.append("å…³é”®è¯åŒ¹é…")
            
        return " + ".join(components) if components else "åŸºç¡€åŒ¹é…"


class IntelligentRetrievalEngine:
    """æ™ºèƒ½æ£€ç´¢å¼•æ“ä¸»ç±»"""
    
    def __init__(self, memory_provider):
        self.memory_provider = memory_provider
        self.semantic_analyzer = AdvancedSemanticAnalyzer()
        self.scoring_algorithm = HybridScoringAlgorithm()
        
        # åˆå§‹åŒ–æ··åˆé‡æ’åºå™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.hybrid_reranker = None
        if RERANKER_AVAILABLE:
            try:
                self.hybrid_reranker = HybridReranker()
                logger.info("Hybrid reranker initialized successfully")
            except Exception as e:
                logger.warning(f"Failed to initialize hybrid reranker: {e}")
        
        # æ£€ç´¢é…ç½®
        self.config = {
            'max_results': 10,
            'min_similarity_threshold': 0.3,
            'diversity_factor': 0.7,  # ç»“æœå¤šæ ·æ€§å› å­
            'quality_threshold': 0.5,  # è´¨é‡é˜ˆå€¼
            'enable_neural_rerank': True,  # å¯ç”¨ç¥ç»ç½‘ç»œé‡æ’åº
            'rerank_batch_size': 20,  # é‡æ’åºæ‰¹å¤§å°
        }
        
        # ç¼“å­˜æœºåˆ¶
        self.query_cache = {}
        self.cache_expiry = timedelta(minutes=30)
        
    async def intelligent_retrieve(
        self,
        query: str,
        strategy: RetrievalStrategy = RetrievalStrategy.HYBRID_ADVANCED,
        session_history: Optional[List[Dict[str, Any]]] = None,
        max_results: int = 5,
        enable_neural_rerank: Optional[bool] = None
    ) -> List[RetrievalResult]:
        """æ™ºèƒ½æ£€ç´¢ä¸»æ–¹æ³•"""
        
        # 1. æŸ¥è¯¢åˆ†æ
        query_context = self.semantic_analyzer.analyze_query(query)
        if session_history:
            query_context.session_history = session_history
            
        logger.info(f"æŸ¥è¯¢åˆ†æ: ç±»å‹={query_context.query_type.value}, "
                   f"æ„å›¾={query_context.user_intent}, ç´§æ€¥åº¦={query_context.urgency_level}")
        
        # 2. æ£€æŸ¥ç¼“å­˜
        use_neural = enable_neural_rerank if enable_neural_rerank is not None else self.config['enable_neural_rerank']
        cache_key = self._generate_cache_key(query, strategy, max_results, use_neural)
        if cache_key in self.query_cache:
            cached_result, timestamp = self.query_cache[cache_key]
            if datetime.now() - timestamp < self.cache_expiry:
                logger.info("ä½¿ç”¨ç¼“å­˜ç»“æœ")
                return cached_result
                
        # 3. åŸºç¡€æ£€ç´¢ï¼ˆè·å–æ›´å¤šç»“æœç”¨äºé‡æ’åºï¼‰
        retrieval_count = max_results * 3 if use_neural and self.hybrid_reranker else max_results * 2
        base_results = await self._perform_base_retrieval(query, retrieval_count)
        
        # 4. æ™ºèƒ½é‡æ’åº
        enhanced_results = await self._intelligent_rerank(base_results, query_context)
        
        # 5. ç¥ç»ç½‘ç»œé‡æ’åºï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if use_neural and self.hybrid_reranker and len(enhanced_results) > 3:
            enhanced_results = await self._apply_neural_rerank(
                query, enhanced_results, query_context
            )
        
        # 6. å¤šæ ·æ€§ä¼˜åŒ–
        final_results = self._apply_diversity_filter(enhanced_results, max_results)
        
        # 7. ç¼“å­˜ç»“æœ
        self.query_cache[cache_key] = (final_results, datetime.now())
        
        logger.info(f"æ™ºèƒ½æ£€ç´¢å®Œæˆ: è¿”å›{len(final_results)}ä¸ªç»“æœ (ç¥ç»é‡æ’åº: {use_neural})")
        return final_results
    
    async def _perform_base_retrieval(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """æ‰§è¡ŒåŸºç¡€æ£€ç´¢"""
        try:
            # ä½¿ç”¨è®°å¿†æä¾›è€…çš„æœç´¢åŠŸèƒ½
            search_results = self.memory_provider.search_memory(query, n=max_results)
            
            results = []
            for result in search_results:
                results.append({
                    'content': result.content,
                    'role': result.role,
                    'similarity_score': result.score,
                    'metadata': result.metadata
                })
            
            return results
            
        except Exception as e:
            logger.error(f"åŸºç¡€æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    async def _intelligent_rerank(
        self,
        base_results: List[Dict[str, Any]],
        query_context: QueryContext
    ) -> List[RetrievalResult]:
        """æ™ºèƒ½é‡æ’åº"""
        enhanced_results = []
        
        for result in base_results:
            # è®¡ç®—ç»¼åˆå¾—åˆ†
            final_score, reasoning = self.scoring_algorithm.calculate_comprehensive_score(
                result, query_context, result['similarity_score']
            )
            
            # åˆ›å»ºå¢å¼ºç»“æœ
            enhanced_result = RetrievalResult(
                content=result['content'],
                role=result['role'],
                similarity_score=result['similarity_score'],
                temporal_score=0.0,  # å°†åœ¨ç»¼åˆå¾—åˆ†ä¸­ä½“ç°
                context_score=0.0,   # å°†åœ¨ç»¼åˆå¾—åˆ†ä¸­ä½“ç°
                final_score=final_score,
                metadata=result.get('metadata', {}),
                reasoning=reasoning
            )
            
            enhanced_results.append(enhanced_result)
        
        # æŒ‰æœ€ç»ˆå¾—åˆ†æ’åº
        enhanced_results.sort(key=lambda x: x.final_score, reverse=True)
        
        return enhanced_results
    
    def _apply_diversity_filter(
        self,
        results: List[RetrievalResult],
        max_results: int
    ) -> List[RetrievalResult]:
        """åº”ç”¨å¤šæ ·æ€§è¿‡æ»¤"""
        if len(results) <= max_results:
            return results
            
        # é€‰æ‹©ç®—æ³•ï¼šè´ªå¿ƒé€‰æ‹©ï¼Œå¹³è¡¡å¾—åˆ†å’Œå¤šæ ·æ€§
        selected = []
        remaining = results.copy()
        
        # 1. é€‰æ‹©å¾—åˆ†æœ€é«˜çš„
        if remaining:
            selected.append(remaining.pop(0))
            
        # 2. å¹³è¡¡é€‰æ‹©å‰©ä½™çš„
        while len(selected) < max_results and remaining:
            best_candidate = None
            best_diversity_score = -1
            
            for i, candidate in enumerate(remaining):
                # è®¡ç®—ä¸å·²é€‰æ‹©ç»“æœçš„å¤šæ ·æ€§
                diversity = self._calculate_diversity(candidate, selected)
                combined_score = (
                    candidate.final_score * (1 - self.config['diversity_factor']) +
                    diversity * self.config['diversity_factor']
                )
                
                if combined_score > best_diversity_score:
                    best_diversity_score = combined_score
                    best_candidate = i
                    
            if best_candidate is not None:
                selected.append(remaining.pop(best_candidate))
            else:
                break
                
        return selected
    
    def _calculate_diversity(
        self,
        candidate: RetrievalResult,
        selected: List[RetrievalResult]
    ) -> float:
        """è®¡ç®—å€™é€‰ç»“æœä¸å·²é€‰ç»“æœçš„å¤šæ ·æ€§"""
        if not selected:
            return 1.0
            
        similarities = []
        candidate_content = candidate.content.lower()
        
        for selected_result in selected:
            selected_content = selected_result.content.lower()
            
            # ç®€å•çš„è¯æ±‡é‡å åº¦è®¡ç®—
            candidate_words = set(candidate_content.split())
            selected_words = set(selected_content.split())
            
            if not candidate_words or not selected_words:
                similarities.append(0.0)
                continue
                
            overlap = len(candidate_words & selected_words)
            union = len(candidate_words | selected_words)
            similarity = overlap / union if union > 0 else 0.0
            similarities.append(similarity)
            
        # è¿”å›å¹³å‡ç›¸ä¼¼åº¦çš„è¡¥æ•°ï¼ˆå¤šæ ·æ€§ï¼‰
        avg_similarity = statistics.mean(similarities) if similarities else 0.0
        return 1.0 - avg_similarity
    
    async def _apply_neural_rerank(
        self,
        query: str,
        results: List[RetrievalResult],
        query_context: QueryContext
    ) -> List[RetrievalResult]:
        """åº”ç”¨ç¥ç»ç½‘ç»œé‡æ’åº"""
        try:
            # è½¬æ¢ä¸ºé‡æ’åºå™¨æœŸæœ›çš„æ ¼å¼
            rerank_input = []
            for result in results:
                rerank_input.append({
                    'content': result.content,
                    'role': result.role,
                    'final_score': result.final_score,
                    'metadata': result.metadata
                })
            
            # æ‰§è¡Œæ··åˆé‡æ’åº
            reranked = await self.hybrid_reranker.hybrid_rerank(
                query=query,
                retrieval_results=rerank_input,
                query_type=query_context.query_type.value,
                enable_neural=True
            )
            
            # è½¬æ¢å› RetrievalResult æ ¼å¼
            reranked_results = []
            for item in reranked:
                # æ‰¾åˆ°å¯¹åº”çš„åŸå§‹ç»“æœ
                for original in results:
                    if (original.content == item['content'] and 
                        original.role == item['role']):
                        # æ›´æ–°å¾—åˆ†å’Œæ¨ç†
                        original.final_score = item.get('rerank_score', original.final_score)
                        original.reasoning += f" + ç¥ç»é‡æ’åº({item.get('rerank_score', 0):.3f})"
                        reranked_results.append(original)
                        break
            
            return reranked_results if reranked_results else results
            
        except Exception as e:
            logger.error(f"Neural reranking failed: {e}")
            return results
    
    def _generate_cache_key(self, query: str, strategy: RetrievalStrategy, max_results: int, use_neural: bool = False) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{query}_{strategy.value}_{max_results}_{use_neural}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def get_retrieval_stats(self) -> Dict[str, Any]:
        """è·å–æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'cache_size': len(self.query_cache),
            'cache_hit_rate': 0.0,  # TODO: å®ç°ç¼“å­˜å‘½ä¸­ç‡ç»Ÿè®¡
            'config': self.config.copy(),
            'reranker_available': RERANKER_AVAILABLE,
            'reranker_enabled': self.config.get('enable_neural_rerank', False)
        }
        
        if self.hybrid_reranker:
            stats['fusion_configs'] = self.hybrid_reranker.fusion_configs
            
        return stats
    
    def update_config(self, config: Dict[str, Any]):
        """æ›´æ–°é…ç½®"""
        self.config.update(config)
        logger.info(f"IntelligentRetrievalEngine config updated: {config}")


# å·¥å‚å‡½æ•°
def create_intelligent_retrieval_engine(memory_provider) -> IntelligentRetrievalEngine:
    """åˆ›å»ºæ™ºèƒ½æ£€ç´¢å¼•æ“å®ä¾‹"""
    return IntelligentRetrievalEngine(memory_provider)